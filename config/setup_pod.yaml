apiVersion: v1
kind: Pod
metadata:
  name: vae-pod
spec:
  containers:
  - name: gpu-container
    image: timttu/vae-pneumonia:latest
    command: ["/bin/bash", "-c"]
    args: [
      "set -e && \
          echo '=== System Info ===' && \
          free -h && \
          nvidia-smi && \
          echo '=== Installing dependencies ===' && \
          apt-get update && apt-get install -y unzip && \
          pip3 install 'numpy<2.0' gdown --force-reinstall jupyterlab psutil && \
          git config --global credential.helper store && \
          echo \"https://oauth2:${GIT_ACCESS_TOKEN}@github.com\" > ~/.git-credentials && \
          cd /mnt/tim && \
          if [ ! -d 'vlmap' ]; then \
            echo 'Cloning Tim branch...' && \
            git clone -b Tim https://github.com/hwcao17/vlmap.git vlmap ; \
          else \
            echo 'Repository exists, updating Tim branch...' && \
            cd vlmap && \
            git fetch origin && \
            git checkout Tim && \
            git pull origin Tim ; \
          fi && \
          echo '=== Memory Status Before Training ===' && \
          free -h && \
          echo 'Repository cloned/updated. Starting training...' && \
          cd /mnt/tim/vlmap/mapping && \
          mkdir -p logs && \
          LOGFILE=logs/train_multi-level_autodecoder_$(date +%Y%m%d_%H%M%S).log && \
          echo \"Saving logs to $LOGFILE\" && \
          torchrun --standalone --nproc_per_node=gpu train_multi-level_autoencoder.py --cfg_file /mnt/tim/vlmap/configs/local/mapping/train_auto_encoder.yaml 2>&1 | tee $LOGFILE && \
          echo \"Training completed. Logs saved to $LOGFILE\""
    ]
    env:
    - name: AUTO_MODE
      value: "true"
    - name: GIT_ACCESS_TOKEN
      valueFrom:
        secretKeyRef:
          name: tim-github-token
          key: GIT_ACCESS_TOKEN
    volumeMounts:
    - mountPath: /pers_vol
      name: tim-vol
    - mountPath: /dev/shm
      name: dshm
    resources:
      limits:
        nvidia.com/gpu: "1"
        memory: "32G"
        cpu: "4"
      requests:
        nvidia.com/gpu: "1"
        memory: "32G"
        cpu: "3"
  restartPolicy: Never
  affinity:
    nodeAffinity:
      # Exclude nodes with CSI driver issues
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: kubernetes.io/hostname
            operator: NotIn
            values:
            - "k8s-haosu-04.sdsc.optiputer.net"
      preferredDuringSchedulingIgnoredDuringExecution:
      # Empirically determined nodes with good network to download large docker images
      - weight: 100
        preference:
          matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - "ry-gpu-01.sdsc.optiputer.net"
            - "ry-gpu-02.sdsc.optiputer.net"
            - "ry-gpu-03.sdsc.optiputer.net"
            - "ry-gpu-04.sdsc.optiputer.net"
            - "ry-gpu-05.sdsc.optiputer.net"
            - "ry-gpu-06.sdsc.optiputer.net"
            - "ry-gpu-07.sdsc.optiputer.net"
            - "ry-gpu-08.sdsc.optiputer.net"
            - "ry-gpu-09.sdsc.optiputer.net"
            - "ry-gpu-10.sdsc.optiputer.net"
            - "ry-gpu-11.sdsc.optiputer.net"
            - "ry-gpu-12.sdsc.optiputer.net"
            - "ry-gpu-13.sdsc.optiputer.net"
            - "ry-gpu-14.sdsc.optiputer.net"
            - "ry-gpu-15.sdsc.optiputer.net"
            - "ry-gpu-16.sdsc.optiputer.net"
            - "k8s-3090-01.calit2.optiputer.net"
            - "k8s-4090-01.calit2.optiputer.net"
            - "k8s-4090-02.calit2.optiputer.net"
      - weight: 50
        preference:
          matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
            - "k8s-haosu-02.sdsc.optiputer.net"
            - "k8s-haosu-03.sdsc.optiputer.net"
            - "k8s-haosu-04.sdsc.optiputer.net"
            - "k8s-haosu-05.sdsc.optiputer.net"
            - "k8s-haosu-06.sdsc.optiputer.net"
            - "k8s-haosu-07.sdsc.optiputer.net"
            - "k8s-haosu-08.sdsc.optiputer.net"
            - "k8s-haosu-09.sdsc.optiputer.net"
            - "k8s-haosu-10.sdsc.optiputer.net"
            - "k8s-haosu-11.sdsc.optiputer.net"
            - "k8s-haosu-12.sdsc.optiputer.net"
            - "k8s-haosu-13.sdsc.optiputer.net"
            - "k8s-haosu-14.sdsc.optiputer.net"
            - "k8s-haosu-15.sdsc.optiputer.net"
            - "k8s-haosu-16.sdsc.optiputer.net"
            - "k8s-haosu-17.sdsc.optiputer.net"
            - "k8s-haosu-18.sdsc.optiputer.net"
            - "k8s-haosu-19.sdsc.optiputer.net"
            - "k8s-haosu-20.sdsc.optiputer.net"
            - "k8s-haosu-21.sdsc.optiputer.net"
            - "k8s-haosu-22.sdsc.optiputer.net"
            - "k8s-haosu-23.sdsc.optiputer.net"
      - weight: 80
        preference:
          matchExpressions:
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - "NVIDIA-A100-80GB-PCIe"
            - "NVIDIA-A100-SXM4-80GB"
            - "NVIDIA-A100-80GB-PCIe-MIG-1g.10gb"
            - "NVIDIA-A100-PCIE-40GB"
            - "NVIDIA-RTX-A6000"
            - "NVIDIA-RTX-A5000"
            - "NVIDIA-GeForce-RTX-4090"
            - "NVIDIA-GeForce-RTX-4080"
            - "NVIDIA-GeForce-RTX-3090-Ti"
            - "NVIDIA-GeForce-RTX-3090"
            - "NVIDIA-GeForce-RTX-3080-Ti"
            - "NVIDIA-GeForce-RTX-3080"
      - weight: 60
        preference:
          matchExpressions:
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - "NVIDIA-A40"
            - "Tesla-V100-SXM2-32GB"
            - "Tesla-V100-PCIE-16GB"
      - weight: 50
        preference:
          matchExpressions:
          - key: nvidia.com/gpu.product
            operator: In
            values:
            - "NVIDIA-TITAN-RTX"
            - "NVIDIA-GeForce-RTX-2080-Ti"
            - "NVIDIA-TITAN-Xp"
            - "NVIDIA-L40"
            - "NVIDIA-A10"
  volumes:
    - name: tim-vol
      persistentVolumeClaim:
        claimName: tim-vol
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 8Gi



